#!/usr/bin/env nix-shell
#!nix-shell -i python3 -p python3Packages.requests python3Packages.mutagen python3Packages.blake3
"""
tidal-dl — Download your entire Tidal library at MAX (HI_RES_LOSSLESS) quality.

Downloads:
  - All favorite albums (full track listings)
  - All favorite tracks (loose, into a _Favorites folder)
  - All playlists (into _Playlists/<name>/)

Files go to: ~/Music/Tidal/<Artist>/<Album>/<NN> - <Title>.<ext>
Skips already-downloaded files. Tags everything with metadata.
"""

import base64
import json
import os
import re
import shutil
import subprocess
import sys
import time
from pathlib import Path

import blake3
import requests
from mutagen.flac import FLAC
from mutagen.mp4 import MP4

# ── Config ────────────────────────────────────────────────────────────────────

CREDS_PATH = Path.home() / ".config" / "tidal-tui" / "credentials.json"
DEFAULT_OUTPUT_DIR = Path.home() / "Music" / "Tidal"
OUTPUT_DIR = DEFAULT_OUTPUT_DIR  # overridden by --output
CLIENT_ID = "dN2N95wCyEBTllu4"
API_BASE = "https://api.tidal.com/v1"
AUTH_URL = "https://auth.tidal.com/v1/oauth2/token"
COUNTRY = "US"
PAGE_SIZE = 100

# Quality cascade — try best first, fall back
QUALITY_CASCADE = ["HI_RES_LOSSLESS", "HI_RES", "LOSSLESS", "HIGH"]

# Rate limiting
REQUEST_DELAY = 0.3  # seconds between API calls
DOWNLOAD_DELAY = 0.5  # seconds between track downloads

# ── State ─────────────────────────────────────────────────────────────────────

session = requests.Session()
creds = {}
stats = {"downloaded": 0, "skipped": 0, "failed": 0, "total_bytes": 0}

# Content-addressed dedup backed by redb (via tidal-db co-process).
# Two indices: track_id for fast pre-download lookups, BLAKE3 hash for
# post-download content dedup. The redb database gives us ACID writes,
# instant startup, and zero JSON serialization overhead.
db = None  # TidalDb instance, set in main()
seen_this_run = set()  # track IDs processed this session


# ── Helpers ───────────────────────────────────────────────────────────────────

def sanitize(name: str) -> str:
    """Sanitize a string for use as a filename."""
    name = re.sub(r'[<>:"/\\|?*]', "_", name)
    name = name.strip(". ")
    return name or "_"


def fmt_bytes(n: int) -> str:
    for unit in ("B", "KB", "MB", "GB", "TB"):
        if n < 1024:
            return f"{n:.1f} {unit}"
        n /= 1024
    return f"{n:.1f} PB"


def fmt_duration(secs: int) -> str:
    m, s = divmod(secs, 60)
    return f"{m}:{s:02d}"


# ── Download History (content-addressed with BLAKE3, backed by redb) ──────────

class TidalDb:
    """Wraps the tidal-db Rust co-process for redb-backed download history."""

    def __init__(self, db_path: Path):
        binary = self._find_binary()
        self.proc = subprocess.Popen(
            [str(binary), "serve", str(db_path)],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            text=True,
            bufsize=1,  # line-buffered
        )

    @staticmethod
    def _find_binary() -> Path:
        script_dir = Path(__file__).resolve().parent
        # Check CARGO_TARGET_DIR first, then local target/, then PATH
        cargo_target = os.environ.get("CARGO_TARGET_DIR")
        if cargo_target:
            p = Path(cargo_target) / "release" / "tidal-db"
            if p.exists():
                return p
        for profile in ("release", "debug"):
            p = script_dir / "target" / profile / "tidal-db"
            if p.exists():
                return p
        p = shutil.which("tidal-db")
        if p:
            return Path(p)
        print("✗ tidal-db binary not found. Run: cargo build --release --bin tidal-db")
        sys.exit(1)

    def _cmd(self, cmd: dict) -> dict:
        self.proc.stdin.write(json.dumps(cmd) + "\n")
        self.proc.stdin.flush()
        line = self.proc.stdout.readline()
        return json.loads(line)

    def check(self, track_id: str) -> dict:
        return self._cmd({"cmd": "check", "track_id": str(track_id)})

    def check_batch(self, track_ids: list[str]) -> set[str]:
        """Batch check track IDs. Returns set of IDs that exist with valid files."""
        result = self._cmd({"cmd": "check_batch", "track_ids": [str(t) for t in track_ids]})
        return set(result.get("found", []))

    def check_album(self, album_id) -> bool:
        """Check if an album is marked as fully downloaded."""
        return self._cmd({"cmd": "check_album", "album_id": str(album_id)}).get("complete", False)

    def mark_album(self, album_id):
        """Mark an album as fully downloaded."""
        return self._cmd({"cmd": "mark_album", "album_id": str(album_id)})

    def check_hash(self, blake3_hash: str) -> dict:
        return self._cmd({"cmd": "check_hash", "hash": blake3_hash})

    def put(self, track_id: str, blake3_hash: str, path: str, artist: str, title: str):
        return self._cmd({
            "cmd": "put",
            "track_id": str(track_id),
            "hash": blake3_hash,
            "path": path,
            "artist": artist,
            "title": title,
        })

    def stats(self) -> dict:
        return self._cmd({"cmd": "stats"})

    def import_json(self, json_path: str) -> dict:
        return self._cmd({"cmd": "import", "json_path": json_path})

    def prune(self) -> dict:
        return self._cmd({"cmd": "prune"})

    def close(self):
        if self.proc and self.proc.poll() is None:
            self.proc.stdin.close()
            self.proc.wait()


def hash_file(path: Path) -> str:
    """Compute BLAKE3 hash of a file."""
    hasher = blake3.blake3()
    with open(path, "rb") as f:
        while chunk := f.read(65536):
            hasher.update(chunk)
    return hasher.hexdigest()


# ── Auth ──────────────────────────────────────────────────────────────────────

def load_creds():
    global creds
    if not CREDS_PATH.exists():
        print(f"✗ No credentials found at {CREDS_PATH}")
        print("  Run drift first to authenticate with Tidal.")
        sys.exit(1)
    creds = json.loads(CREDS_PATH.read_text())
    session.headers["Authorization"] = f"Bearer {creds['access_token']}"
    print(f"✓ Loaded credentials (user_id: {creds['user_id']})")


def save_creds():
    CREDS_PATH.write_text(json.dumps(creds, indent=2))


def refresh_token():
    global creds
    print("  ↻ Refreshing access token...")
    resp = requests.post(AUTH_URL, data={
        "grant_type": "refresh_token",
        "refresh_token": creds["refresh_token"],
        "client_id": CLIENT_ID,
    })
    if resp.status_code != 200:
        print(f"✗ Token refresh failed: {resp.status_code} {resp.text}")
        sys.exit(1)
    data = resp.json()
    creds["access_token"] = data["access_token"]
    if "refresh_token" in data:
        creds["refresh_token"] = data["refresh_token"]
    session.headers["Authorization"] = f"Bearer {creds['access_token']}"
    save_creds()
    print("  ✓ Token refreshed")


# ── API ───────────────────────────────────────────────────────────────────────

def api_get(path: str, params: dict = None, retry=True) -> dict:
    """Make an authenticated GET request to the Tidal API."""
    url = f"{API_BASE}/{path}" if not path.startswith("http") else path
    base_params = {"countryCode": COUNTRY}
    if params:
        base_params.update(params)

    time.sleep(REQUEST_DELAY)
    resp = session.get(url, params=base_params)

    if resp.status_code == 401 and retry:
        refresh_token()
        return api_get(path, params, retry=False)

    if not resp.ok:
        print(f"  ✗ API error: {resp.status_code} {path}")
        return {}

    return resp.json()


def paginate(path: str, key: str = "items", params: dict = None) -> list:
    """Fetch all pages of a paginated endpoint."""
    all_items = []
    offset = 0
    while True:
        p = {"limit": str(PAGE_SIZE), "offset": str(offset)}
        if params:
            p.update(params)
        data = api_get(path, p)
        items = data.get(key, [])
        if not items:
            break
        all_items.extend(items)
        total = data.get("totalNumberOfItems", len(all_items))
        if len(all_items) >= total:
            break
        offset += PAGE_SIZE
    return all_items


# ── Library Fetch ─────────────────────────────────────────────────────────────

def get_favorite_albums() -> list:
    """Fetch all favorite albums."""
    items = paginate(f"users/{creds['user_id']}/favorites/albums")
    albums = []
    for item in items:
        album = item.get("item", item)
        artist = album.get("artist", {}).get("name") or album.get("artists", [{}])[0].get("name", "Unknown Artist")
        albums.append({
            "id": album["id"],
            "title": album.get("title", "Unknown Album"),
            "artist": artist,
            "num_tracks": album.get("numberOfTracks", 0),
            "cover": album.get("cover"),
        })
    return albums


def get_favorite_tracks() -> list:
    """Fetch all favorite tracks."""
    items = paginate(f"users/{creds['user_id']}/favorites/tracks")
    return [parse_track(item.get("item", item)) for item in items if parse_track(item.get("item", item))]


def get_playlists() -> list:
    """Fetch all user playlists."""
    items = paginate(f"users/{creds['user_id']}/playlists")
    return [{
        "id": p.get("uuid", p.get("id")),
        "title": p.get("title", "Untitled"),
        "num_tracks": p.get("numberOfTracks", 0),
    } for p in items]


def get_album_tracks(album_id) -> list:
    """Fetch all tracks in an album."""
    items = paginate(f"albums/{album_id}/items")
    tracks = []
    for item in items:
        track = parse_track(item.get("item", item))
        if track:
            tracks.append(track)
    return tracks


def get_playlist_tracks(playlist_id: str) -> list:
    """Fetch all tracks in a playlist."""
    items = paginate(f"playlists/{playlist_id}/items")
    tracks = []
    for item in items:
        track = parse_track(item.get("item", item))
        if track:
            tracks.append(track)
    return tracks


def parse_track(data: dict) -> dict | None:
    """Parse a track object from the API."""
    if not data or "id" not in data:
        return None
    artist = (
        data.get("artist", {}).get("name")
        or (data.get("artists", [{}])[0].get("name") if data.get("artists") else None)
        or "Unknown Artist"
    )
    album = data.get("album", {})
    return {
        "id": data["id"],
        "title": data.get("title", "Unknown"),
        "artist": artist,
        "album": album.get("title", "Unknown Album"),
        "album_artist": album.get("artist", {}).get("name", artist),
        "duration": data.get("duration", 0),
        "track_number": data.get("trackNumber", 0),
        "volume_number": data.get("volumeNumber", 1),
        "cover": album.get("cover"),
    }


# ── Download ──────────────────────────────────────────────────────────────────

def get_stream_url(track_id: int) -> tuple[str, str]:
    """
    Get the stream URL for a track at the highest available quality.
    Returns (url, codec) tuple.
    """
    for quality in QUALITY_CASCADE:
        data = api_get(f"tracks/{track_id}/playbackinfo", {
            "audioquality": quality,
            "assetpresentation": "FULL",
            "playbackmode": "STREAM",
        })

        if not data:
            continue

        codec = data.get("audioQuality", quality)
        manifest_mime = data.get("manifestMimeType", "")
        manifest_b64 = data.get("manifest", "")

        if not manifest_b64:
            # Try direct URL field
            url = data.get("url")
            if url:
                return url, codec
            continue

        # Decode manifest
        try:
            manifest_str = base64.b64decode(manifest_b64).decode("utf-8")
        except Exception:
            continue

        if "application/dash+xml" in manifest_mime:
            # DASH manifest — extract URL from XML
            import re as _re
            urls = _re.findall(r'<BaseURL>(.*?)</BaseURL>', manifest_str)
            if urls:
                return urls[0], codec
        else:
            # JSON manifest
            try:
                manifest = json.loads(manifest_str)
                urls = manifest.get("urls", [])
                if urls:
                    return urls[0], codec
            except json.JSONDecodeError:
                continue

    raise RuntimeError(f"No stream URL found for track {track_id}")


def file_extension(codec: str) -> str:
    """Determine file extension from codec/quality info."""
    codec_lower = codec.lower()
    if "flac" in codec_lower or "lossless" in codec_lower or "hi_res" in codec_lower:
        return "flac"
    if "aac" in codec_lower or "mp4" in codec_lower:
        return "m4a"
    if "mp3" in codec_lower:
        return "mp3"
    # Default to FLAC for high quality
    return "flac"


def download_track(track: dict, dest_dir: Path, index: int = 0, total: int = 0,
                   use_index_as_num: bool = False, known_ids: set = None) -> bool:
    """
    Download a single track. Returns True if downloaded, False if skipped.
    Deduplicates by track ID (pre-download) and BLAKE3 content hash (post-download)
    across all directories (albums, favorites, playlists).
    Pass known_ids (from check_batch) to skip individual db lookups.
    """
    track_id = str(track["id"])

    # Fast path — skip if already processed this run
    if track_id in seen_this_run:
        stats["skipped"] += 1
        return False

    # Skip if batch pre-check (or individual check) says we have it
    if known_ids is not None:
        if track_id in known_ids:
            stats["skipped"] += 1
            seen_this_run.add(track_id)
            return False
    else:
        result = db.check(track_id)
        if result.get("exists"):
            stats["skipped"] += 1
            seen_this_run.add(track_id)
            return False

    track_num = index if use_index_as_num else track.get("track_number", index)
    title = sanitize(track["title"])
    artist = sanitize(track["artist"])

    # Check for existing files with any extension
    prefix = f"{track_num:02d} - {title}"
    for ext in ("flac", "m4a", "mp3"):
        existing = dest_dir / f"{prefix}.{ext}"
        if existing.exists():
            stats["skipped"] += 1
            # Hash and record in redb so cross-directory dedup works
            content_hash = hash_file(existing)
            db.put(track_id, content_hash, str(existing), track["artist"], track["title"])
            seen_this_run.add(track_id)
            return False

    # Get stream URL
    try:
        url, codec = get_stream_url(track["id"])
    except RuntimeError as e:
        print(f"    ✗ {prefix}: {e}")
        stats["failed"] += 1
        return False

    ext = file_extension(codec)
    dest = dest_dir / f"{prefix}.{ext}"
    dest_dir.mkdir(parents=True, exist_ok=True)

    # Download
    progress_label = f"[{index}/{total}]" if total else ""
    print(f"    ↓ {progress_label} {track['artist']} - {track['title']} [{codec}]")

    try:
        time.sleep(DOWNLOAD_DELAY)
        resp = session.get(url, stream=True, timeout=(10, 60))
        resp.raise_for_status()

        # Stream to disk while computing BLAKE3 hash in one pass
        hasher = blake3.blake3()
        downloaded = 0

        with open(dest, "wb") as f:
            for chunk in resp.iter_content(chunk_size=65536):
                f.write(chunk)
                hasher.update(chunk)
                downloaded += len(chunk)

        content_hash = hasher.hexdigest()

        # Post-download content dedup — if we already have this exact content, discard
        hash_result = db.check_hash(content_hash)
        if hash_result.get("exists"):
            existing_path = hash_result.get("path", "?")
            print(f"    ≡ Duplicate content (matches {Path(existing_path).name}), removing")
            dest.unlink()
            stats["skipped"] += 1
            db.put(track_id, content_hash, existing_path, track["artist"], track["title"])
            seen_this_run.add(track_id)
            return False

        stats["downloaded"] += 1
        stats["total_bytes"] += downloaded

        # Tag the file
        tag_file(dest, track, ext)

        # Record in redb — ACID write, no periodic save needed
        db.put(track_id, content_hash, str(dest), track["artist"], track["title"])
        seen_this_run.add(track_id)

        return True

    except Exception as e:
        print(f"    ✗ Download failed: {e}")
        if dest.exists():
            dest.unlink()
        stats["failed"] += 1
        return False


def tag_file(path: Path, track: dict, ext: str):
    """Write metadata tags to the downloaded file."""
    try:
        if ext == "flac":
            audio = FLAC(str(path))
            audio["TITLE"] = track["title"]
            audio["ARTIST"] = track["artist"]
            audio["ALBUM"] = track["album"]
            audio["ALBUMARTIST"] = track.get("album_artist", track["artist"])
            if track.get("track_number"):
                audio["TRACKNUMBER"] = str(track["track_number"])
            if track.get("volume_number"):
                audio["DISCNUMBER"] = str(track["volume_number"])
            audio.save()

        elif ext == "m4a":
            audio = MP4(str(path))
            audio["\xa9nam"] = [track["title"]]
            audio["\xa9ART"] = [track["artist"]]
            audio["\xa9alb"] = [track["album"]]
            audio["aART"] = [track.get("album_artist", track["artist"])]
            if track.get("track_number"):
                audio["trkn"] = [(track["track_number"], 0)]
            if track.get("volume_number"):
                audio["disk"] = [(track["volume_number"], 0)]
            audio.save()

    except Exception as e:
        print(f"    ⚠ Tagging failed: {e}")


# ── Main ──────────────────────────────────────────────────────────────────────

def download_albums(albums: list):
    """Download all tracks from a list of albums."""
    cached = 0
    for i, album in enumerate(albums, 1):
        artist_dir = sanitize(album["artist"])
        album_dir = sanitize(album["title"])
        dest = OUTPUT_DIR / artist_dir / album_dir

        # Album-level cache — skip API call entirely if we know it's complete
        if db.check_album(album["id"]):
            cached += 1
            continue

        print(f"\n  [{i}/{len(albums)}] {album['artist']} — {album['title']} ({album['num_tracks']} tracks)")

        tracks = get_album_tracks(album["id"])
        if not tracks:
            print("    ⚠ No tracks found")
            continue

        # Batch check all track IDs in one IPC call
        track_ids = [str(t["id"]) for t in tracks]
        known = db.check_batch(track_ids)

        failed_before = stats["failed"]
        new = 0
        for j, track in enumerate(tracks, 1):
            if download_track(track, dest, index=j, total=len(tracks), known_ids=known):
                new += 1

        album_failed = stats["failed"] - failed_before
        if new == 0:
            print("    ✓ Already complete")

        # Mark album complete so we skip the API call next run
        if album_failed == 0 and len(known & set(track_ids)) + new == len(tracks):
            db.mark_album(album["id"])

    if cached:
        print(f"\n  ✓ {cached} albums skipped (cached as complete)")


def download_loose_tracks(tracks: list, dest: Path, label: str = "Favorites"):
    """Download a list of tracks into a flat directory."""
    if not tracks:
        return

    # Batch check all track IDs up front
    track_ids = [str(t["id"]) for t in tracks]
    known = db.check_batch(track_ids)
    new_count = len(track_ids) - len(known & set(track_ids))

    print(f"\n  {label} ({len(tracks)} tracks, {new_count} new)")
    if new_count == 0:
        print("    ✓ All tracks already downloaded")
        for tid in track_ids:
            seen_this_run.add(tid)
        stats["skipped"] += len(track_ids)
        return

    for i, track in enumerate(tracks, 1):
        # For loose tracks, use artist subdirectories and sequential numbering
        track_dest = dest / sanitize(track["artist"])
        download_track(track, track_dest, index=i, total=len(tracks),
                       use_index_as_num=True, known_ids=known)


def download_playlists(playlists: list):
    """Download all playlists."""
    for i, pl in enumerate(playlists, 1):
        dest = OUTPUT_DIR / "_Playlists" / sanitize(pl["title"])
        print(f"\n  [{i}/{len(playlists)}] Playlist: {pl['title']} ({pl['num_tracks']} tracks)")

        tracks = get_playlist_tracks(pl["id"])
        if not tracks:
            print("    ⚠ No tracks found")
            continue

        # Batch check all track IDs
        track_ids = [str(t["id"]) for t in tracks]
        known = db.check_batch(track_ids)

        for j, track in enumerate(tracks, 1):
            download_track(track, dest, index=j, total=len(tracks),
                           use_index_as_num=True, known_ids=known)


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Download your Tidal library at MAX quality.")
    parser.add_argument("-o", "--output", type=Path, default=DEFAULT_OUTPUT_DIR,
                        help=f"Output directory (default: {DEFAULT_OUTPUT_DIR})")
    args = parser.parse_args()

    global OUTPUT_DIR, db
    OUTPUT_DIR = args.output.resolve()

    print("╔══════════════════════════════════════════╗")
    print("║   tidal-dl — MAX quality library dump    ║")
    print("╚══════════════════════════════════════════╝")
    print()

    load_creds()

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    print(f"✓ Output directory: {OUTPUT_DIR}")

    # Open redb-backed download history
    db_path = OUTPUT_DIR / ".tidal-dl.redb"
    db = TidalDb(db_path)

    # One-time migration: import old JSON history if it exists
    json_history = OUTPUT_DIR / ".tidal-dl-history.json"
    if json_history.exists():
        result = db.stats()
        if result.get("count", 0) == 0:
            print("  ↻ Migrating JSON history to redb...")
            imp = db.import_json(str(json_history))
            print(f"  ✓ Imported {imp.get('imported', 0)} tracks")
        # Keep the JSON file around as backup (rename it)
        backup = json_history.with_suffix(".json.bak")
        json_history.rename(backup)
        print(f"  ✓ Old history backed up to {backup.name}")

    result = db.stats()
    print(f"  ✓ Download history: {result.get('count', 0)} tracks in redb")

    start_time = time.time()

    # ── Fetch library ─────────────────────────────────────────────────────
    print("\n━━━ Fetching library ━━━")

    print("  Fetching favorite albums...", end=" ", flush=True)
    albums = get_favorite_albums()
    print(f"{len(albums)} albums")

    print("  Fetching favorite tracks...", end=" ", flush=True)
    fav_tracks = get_favorite_tracks()
    print(f"{len(fav_tracks)} tracks")

    print("  Fetching playlists...", end=" ", flush=True)
    playlists = get_playlists()
    print(f"{len(playlists)} playlists")

    total_tracks = sum(a["num_tracks"] for a in albums) + len(fav_tracks) + sum(p["num_tracks"] for p in playlists)
    print(f"\n  Total: ~{total_tracks} tracks to process")

    # ── Download ──────────────────────────────────────────────────────────
    print("\n━━━ Downloading albums ━━━")
    download_albums(albums)

    print("\n━━━ Downloading favorite tracks ━━━")
    download_loose_tracks(fav_tracks, OUTPUT_DIR / "_Favorites", "Favorite Tracks")

    print("\n━━━ Downloading playlists ━━━")
    download_playlists(playlists)

    # ── Summary ───────────────────────────────────────────────────────────
    elapsed = time.time() - start_time
    mins, secs = divmod(int(elapsed), 60)

    print("\n━━━ Done ━━━")
    print(f"  Downloaded: {stats['downloaded']} tracks ({fmt_bytes(stats['total_bytes'])})")
    print(f"  Skipped:    {stats['skipped']} (already existed)")
    print(f"  Failed:     {stats['failed']}")
    print(f"  Time:       {mins}m {secs}s")
    print(f"  Location:   {OUTPUT_DIR}")

    db.close()


if __name__ == "__main__":
    main()
